{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get_data.py', 'data-preprocessing.ipynb', 'kaggle_0.csv', 'EDA.ipynb', 'testing-kaggle-submission-conv-and-lstm.ipynb', 'testing-kaggle-submission.ipynb', 'data', '.git', 'kaggle_2.csv', 'kaggle_3.csv', '.ipynb_checkpoints', 'README.md', 'models', 'results', 'kaggle_gru_1.csv', 'testing-kaggle-submission-Recurrent.ipynb', 'test-json.json', '.gitignore', 'best_model.h5', 'kaggle_lstm_1.csv', 'deep_learning_methods_project_3.ipynb', 'kaggle_4.csv', 'architectures-benchmark.ipynb', 'data-preprocessing-Copy1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import gc\n",
    "import re\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "train_data_path = \"data/train/train/audio/\"\n",
    "test_data_path = \"data/test/test/audio/\"\n",
    "\n",
    "print(os.listdir(\"./\"))\n",
    "\n",
    "train_labels = 'yes no up down left right on off stop go silence unknown'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)(audio, sample_rate, window_size=40,\n",
    "                 step_size=35, eps=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_wavs_fname(dirpath, ext='wav'):\n",
    "    print(dirpath)\n",
    "    fpaths = glob(os.path.join(dirpath, r'*/*' + ext))\n",
    "    pat = r'.+/(\\w+)/\\w+\\.' + ext + '$'\n",
    "    labels = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            labels.append(r.group(1))\n",
    "    pat = r'.+/(\\w+\\.' + ext + ')$'\n",
    "    fnames = []\n",
    "    for fpath in fpaths:\n",
    "        r = re.match(pat, fpath)\n",
    "        if r:\n",
    "            fnames.append(r.group(1))\n",
    "    return labels, fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(samples):\n",
    "    if len(samples) >= L: return samples\n",
    "    else: return np.pad(samples, pad_width=(L - len(samples), 0), mode='constant', constant_values=(0, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_audio(samples, L=16000, num=20):\n",
    "    for i in range(num):\n",
    "        beg = np.random.randint(0, len(samples) - L)\n",
    "        yield samples[beg: beg + L]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_transform(labels):\n",
    "    nlabels = []\n",
    "    for label in labels:\n",
    "        if label == '_background_noise_':\n",
    "            nlabels.append('silence')\n",
    "        elif label not in train_labels:\n",
    "            nlabels.append('unknown')\n",
    "        else:\n",
    "            nlabels.append(label)\n",
    "    return pd.get_dummies(pd.Series(nlabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/train/audio/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzys_spalinski/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64841, 99, 81, 1)\n"
     ]
    }
   ],
   "source": [
    "labels, fnames = list_wavs_fname(train_data_path)\n",
    "\n",
    "L = 16000\n",
    "new_sample_rate = 8000\n",
    "Y_train = []\n",
    "X_train = []\n",
    "\n",
    "for label, fname in zip(labels, fnames):\n",
    "    sample_rate, samples = wavfile.read(os.path.join(train_data_path, label, fname))\n",
    "    samples = pad_audio(samples)\n",
    "    if len(samples) > 16000:\n",
    "        n_samples = chop_audio(samples)\n",
    "    else: n_samples = [samples]\n",
    "    for samples in n_samples:\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / sample_rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        Y_train.append(label)\n",
    "        X_train.append(specgram)\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(tuple(list(X_train.shape) + [1]))\n",
    "Y_train = label_transform(Y_train)\n",
    "label_index = Y_train.columns.values\n",
    "Y_train = Y_train.values\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "del labels, fnames\n",
    "gc.collect()\n",
    "\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_generator(batch=16):\n",
    "    fpaths = glob(os.path.join(test_data_path, '*wav'))\n",
    "    i = 0\n",
    "    for path in fpaths:\n",
    "        if i == 0:\n",
    "            imgs = []\n",
    "            fnames = []\n",
    "        i += 1\n",
    "        rate, samples = wavfile.read(path)\n",
    "        samples = pad_audio(samples)\n",
    "        resampled = signal.resample(samples, int(new_sample_rate / rate * samples.shape[0]))\n",
    "        _, _, specgram = log_specgram(resampled, sample_rate=new_sample_rate)\n",
    "        imgs.append(specgram)\n",
    "        fnames.append(path.split('\\\\')[-1])\n",
    "        if i == batch:\n",
    "            i = 0\n",
    "            imgs = np.array(imgs)\n",
    "            imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "            yield fnames, imgs\n",
    "    if i < batch:\n",
    "        imgs = np.array(imgs)\n",
    "        imgs = imgs.reshape(tuple(list(imgs.shape) + [1]))\n",
    "        yield fnames, imgs\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run()\n",
    "            \n",
    "def validate_architectureCNN(architecture_name,\n",
    "                          num_epochs,\n",
    "                          num_iterations):\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "    \n",
    "        # Reinitializing  weights\n",
    "        # reinitializing weights not working on CPU\n",
    "        \n",
    "        input_shape = (99, 81, 1)\n",
    "        nclass = len(train_labels)\n",
    "        inp = Input(shape=input_shape)\n",
    "        norm_inp = BatchNormalization()(inp)\n",
    "        img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(norm_inp)\n",
    "        img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "        img_1 = Convolution2D(8, kernel_size=2, activation=activations.relu)(img_1)\n",
    "        img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "        img_1 = Dropout(rate=0.2)(img_1)\n",
    "        img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = Convolution2D(16, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "        img_1 = Dropout(rate=0.2)(img_1)\n",
    "        img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = Convolution2D(32, kernel_size=3, activation=activations.relu)(img_1)\n",
    "        img_1 = MaxPooling2D(pool_size=(2, 2))(img_1)\n",
    "        img_1 = Dropout(rate=0.2)(img_1)\n",
    "        img_1 = Flatten()(img_1)\n",
    "\n",
    "        dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(img_1))\n",
    "        dense_1 = BatchNormalization()(Dense(128, activation=activations.relu)(dense_1))\n",
    "        dense_1 = Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "        model = models.Model(inputs=inp, outputs=dense_1)\n",
    "        opt = optimizers.Adam()\n",
    "\n",
    "        model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                              Y_train,\n",
    "                                                              test_size=0.1)\n",
    "        \n",
    "        mc = ModelCheckpoint('best_model.h5',\n",
    "                     monitor='val_accuracy',\n",
    "                     mode='max',\n",
    "                     verbose=1,\n",
    "                     save_weights_only=True,\n",
    "                     save_best_only=True)\n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(x_valid, y_valid),\n",
    "                            epochs=num_epochs,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[mc])\n",
    "        \n",
    "        history = history.history\n",
    "        \n",
    "        # saving json\n",
    "        with open(f'./results/results_{architecture_name}_{it}', 'w') as fp:\n",
    "            json.dump(history, fp, indent=4)\n",
    "            \n",
    "            \n",
    "        # prediction\n",
    "        index = []\n",
    "        results = []\n",
    "        \n",
    "        for fnames, imgs in tqdm(test_data_generator(batch=32)):\n",
    "            predicts = model.predict(imgs)\n",
    "            predicts = np.argmax(predicts, axis=1)\n",
    "            predicts = [label_index[p] for p in predicts]\n",
    "            index.extend([text.split(\"/\")[-1] for text in fnames])\n",
    "            results.extend(predicts)\n",
    "\n",
    "            df = pd.DataFrame(columns=['fname', 'label'])\n",
    "            df['fname'] = index\n",
    "            df['label'] = results\n",
    "            df.to_csv(f'./results/submission_{architecture_name}_{it}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1824/1824 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.7026\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.84518, saving model to best_model.h5\n",
      "1824/1824 [==============================] - 177s 97ms/step - loss: 0.1265 - accuracy: 0.7026 - val_loss: 0.0662 - val_accuracy: 0.8452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4955it [21:46,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "validate_architecture(\"CNN\", 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run()\n",
    "            \n",
    "def validate_architectureRNN(architecture_name,\n",
    "                          num_epochs,\n",
    "                          num_iterations):\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "    \n",
    "        # Reinitializing  weights\n",
    "        # reinitializing weights not working on CPU\n",
    "        \n",
    "        input_shape = (99, 81)\n",
    "        nclass = len(train_labels)\n",
    "        inp = layers.Input(shape=input_shape)\n",
    "        norm_inp = layers.BatchNormalization()(inp)\n",
    "        lstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(norm_inp)\n",
    "        lstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(norm_inp)\n",
    "        lstm = layers.GlobalMaxPooling1D()(lstm)\n",
    "        dense_1 = layers.Dense(128, activation=activations.relu)(lstm)\n",
    "        dense_out = layers.Dense(nclass, activation=activations.softmax)(dense_1)\n",
    "\n",
    "        model = models.Model(inputs=inp, outputs=dense_out)\n",
    "        opt = optimizers.Adam()\n",
    "\n",
    "        model.compile(optimizer=opt, loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                              Y_train,\n",
    "                                                              test_size=0.1)\n",
    "        \n",
    "        mc = ModelCheckpoint('best_model.h5',\n",
    "                     monitor='val_accuracy',\n",
    "                     mode='max',\n",
    "                     verbose=1,\n",
    "                     save_weights_only=True,\n",
    "                     save_best_only=True)\n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(x_valid, y_valid),\n",
    "                            epochs=num_epochs,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[mc])\n",
    "        \n",
    "        history = history.history\n",
    "        \n",
    "        # saving json\n",
    "        with open(f'./results/results_{architecture_name}_{it}', 'w') as fp:\n",
    "            json.dump(history, fp, indent=4)\n",
    "            \n",
    "            \n",
    "        # prediction\n",
    "        index = []\n",
    "        results = []\n",
    "        \n",
    "        for fnames, imgs in tqdm(test_data_generator(batch=32)):\n",
    "            predicts = model.predict(imgs)\n",
    "            predicts = np.argmax(predicts, axis=1)\n",
    "            predicts = [label_index[p] for p in predicts]\n",
    "            index.extend([text.split(\"/\")[-1] for text in fnames])\n",
    "            results.extend(predicts)\n",
    "\n",
    "            df = pd.DataFrame(columns=['fname', 'label'])\n",
    "            df['fname'] = index\n",
    "            df['label'] = results\n",
    "            df.to_csv(f'./results/submission_{architecture_name}_{it}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 547/1824 [=======>......................] - ETA: 2:07 - loss: 0.1210 - accuracy: 0.7309"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-de0112203d88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate_architectureRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"baseline_test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-baed155a7a79>\u001b[0m in \u001b[0;36mvalidate_architectureRNN\u001b[0;34m(architecture_name, num_epochs, num_iterations)\u001b[0m\n\u001b[1;32m     49\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                             callbacks=[mc])\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validate_architectureRNN(\"BidirectionalLSTM\", 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "\n",
    "def reset_weights(model):\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run()\n",
    "            \n",
    "def validate_architectureCNNandRNN(architecture_name,\n",
    "                          num_epochs,\n",
    "                          num_iterations):\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "    \n",
    "        # Reinitializing  weights\n",
    "        # reinitializing weights not working on CPU\n",
    "        \n",
    "        #TODO\n",
    "    \n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                              Y_train,\n",
    "                                                              test_size=0.1)\n",
    "        \n",
    "        mc = ModelCheckpoint('best_model.h5',\n",
    "                     monitor='val_accuracy',\n",
    "                     mode='max',\n",
    "                     verbose=1,\n",
    "                     save_weights_only=True,\n",
    "                     save_best_only=True)\n",
    "        \n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(x_valid, y_valid),\n",
    "                            epochs=num_epochs,\n",
    "                            shuffle=True,\n",
    "                            callbacks=[mc])\n",
    "        \n",
    "        history = history.history\n",
    "        \n",
    "        # saving json\n",
    "        with open(f'./results/results_{architecture_name}_{it}', 'w') as fp:\n",
    "            json.dump(history, fp, indent=4)\n",
    "            \n",
    "            \n",
    "        # prediction\n",
    "        index = []\n",
    "        results = []\n",
    "        \n",
    "        for fnames, imgs in tqdm(test_data_generator(batch=32)):\n",
    "            predicts = model.predict(imgs)\n",
    "            predicts = np.argmax(predicts, axis=1)\n",
    "            predicts = [label_index[p] for p in predicts]\n",
    "            index.extend([text.split(\"/\")[-1] for text in fnames])\n",
    "            results.extend(predicts)\n",
    "\n",
    "            df = pd.DataFrame(columns=['fname', 'label'])\n",
    "            df['fname'] = index\n",
    "            df['label'] = results\n",
    "            df.to_csv(f'./results/submission_{architecture_name}_{it}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
